{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Wikipedia clickstream data: English Wiki in December 2018    \n",
    "\n",
    "## Graph modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction  \n",
    "\n",
    "This notebook contains graph modeling and data import steps for the English Wikipedia clickstream dataset for December 2018.  \n",
    "This is the 3rd part of a project about the usage patterns of Wikipedia.  \n",
    "The preceding parts are:  \n",
    "1. [Data quality analysis of available datasets](data_quality_analysis.ipynb)\n",
    "2. [Exploratory data analysis of the English Wikipedia clickstream dataset for December 2018](English_Wikipedia_EDA.ipynb)\n",
    "\n",
    "In this notebook, we'll use the insights from the exploratory data analysis of our clickstream dataset to model that data as a graph, and then we'll import the data into a neo4j graph database according to our graph model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook contents:  \n",
    "\n",
    "1. [Introduction](#1.-Introduction)  \n",
    "2. [Notebook setup](#2.-Notebook-setup)  \n",
    "3. [Graph modeling](#3.-Graph-modeling)\n",
    "4. [Neo4j setup on AWS EC2 Ubuntu](#4.-Neo4j-setup-on-AWS-EC2-Ubuntu)   \n",
    "5. [Import data into neo4j](#5.-Import-data-into-neo4j)  \n",
    "   5.1 [Get the clickstream data onto AWS](#5.1-Get-the-clickstream-data-onto-AWS)  \n",
    "   5.2 [Connect this notebook to neo4j](#5.2-Connect-this-notebook-to-neo4j)  \n",
    "   5.3 [Set indices](#5.3-Set-indices)  \n",
    "   5.4 [Load in the data](#5.4-Load-in-the-data)  \n",
    "   5.5 [Check the loaded database](#5.5-Check-the-loaded-database)  \n",
    "   5.6 [Adress outliers](#5.6-Adress-outliers)  \n",
    "6. [Summary](#6.-Summary)\n",
    "7. [Next steps](#7.-Next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Notebook setup  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import  csv\n",
    "\n",
    "from time import sleep\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# custom general helper functions for this project\n",
    "import custom_utils as cu\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload imports as needed\n",
    "importlib.reload(cu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up Pandas options\n",
    "pd.set_option('display.max_columns', 25)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Graph modeling  \n",
    "\n",
    "In our dataset, we have Wikipedia articles and external referers with directed clickstream events connecting them, and we also have the aggregated traffic volumes for those events. In a graph model, we can represent the Wikipedia articles as nodes, and the clickstream events as directed edges. The incoming traffic volume from external referers can be captured as node properties, and the clickstream traffic volumes can be captured as edge properties.  \n",
    "\n",
    "We'll have two types of relationships/edges in the model: LINK_TO and SEARCH_FOR, representing link-based clickstream events and internal search clickstream events. Since internal search clickstream events are just clickstream events from one article to the other without the corresponding link between them, it is impossible to have a LINK_TO and a SEARCH_FOR relationships going in the same direction between two nodes (though they can go in the opposite directions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a quick sketch of our graph model using this neat [arrows tool](http://www.apcjones.com/arrows/) for sketching graphs:  \n",
    "\n",
    "<img src=\"img/arrows_articles_graph_sketch.png\" alt=\"Graph model sketch\" width=\"65%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our Wikipedia clickstream graph model consists of:  \n",
    "- **node**: Article  \n",
    "  - node properties: article title, incoming external traffic volumes by external referer type\n",
    "- **relationships/edges**: LINK_TO and SEARCH_FOR\n",
    "  - both relationships are directed  \n",
    "  - relationship property: traffic volume  \n",
    "- **special cases**: we'll need to adress some caveats for special Wikipedia articles like \"Main Page\", \"Hyphen-minus\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Neo4j setup on AWS EC2 Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Neo4j community edition server  \n",
    "\n",
    "- For installing neo4j on an Ubuntu EC2 instance on AWS, follow the Debian installation instructions [here](https://neo4j.com/docs/operations-manual/current/installation/linux/debian/). \n",
    "- [Run neo4j as a service using `systemctl`](https://neo4j.com/docs/operations-manual/current/installation/linux/systemd/#linux-service-control):  \n",
    "  - To start neo4j: `systemctl start neo4j`  \n",
    "  - To stop neo4j: `systemctl stop neo4j`\n",
    "\n",
    "- run neo4j and tunnel into the neo4j browser:\n",
    "  - run neo4j: `systemctl start neo4j`\n",
    "  - tunnel into the browser, e.g.: `ssh -NfL localhost:7474:localhost:7474 -L localhost:7687:localhost:7687 arinai@<AWS instance public IP address>`\n",
    "  - in a browser, open `http://localhost:7474/browser/`\n",
    "  - On first login, both the username and password are `neo4j`. After the initial login there will be a prompt to set a new password. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Import data into neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Get the clickstream data onto AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have cleaned the clickstream tsv datafile in the exploratory data analysis notebook, we can upload it from local machine onto AWS with `scp` in terminal, e.g.:  \n",
    "```\n",
    "scp ~/projects/wiki_graph/data/clickstream-enwiki-2018-12_clean.tsv arinai@<AWS instance public IP address>:/var/lib/neo4j/import/  \n",
    "```\n",
    "However, this took about 15 min to run, and it would probably be much faster to `wget` the datafile from the [Wikimedia datadump](https://dumps.wikimedia.org/other/clickstream/) and run [the cleaning `sed` command](data_quality_analysis.ipynb#Escape-backslashes-first,-and-then-double-quotes) on it on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** By default, neo4j only imports data from its own `import` subfolder (on Ubuntu, it is `/var/lib/neo4j/import`) or from an url, so make sure to copy/move the data into the neo4j import folder (alternatively, this requirement can be commented out in the `conf` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other-empty\t2019_Horizon_League_Baseball_Tournament\texternal\t16\r\n",
      "other-search\tForeverAtLast\texternal\t40\r\n",
      "other-empty\tForeverAtLast\texternal\t85\r\n",
      "First_Families_of_Pakistan\tJehangir_Wadia\tlink\t19\r\n",
      "The_Lawrence_School,_Sanawar\tJehangir_Wadia\tlink\t36\r\n",
      "Wadia_family\tJehangir_Wadia\tlink\t715\r\n",
      "other-search\tJehangir_Wadia\texternal\t967\r\n",
      "Ness_Wadia\tJehangir_Wadia\tlink\t494\r\n",
      "other-empty\tJehangir_Wadia\texternal\t638\r\n",
      "GoAir\tJehangir_Wadia\tlink\t1191\r\n"
     ]
    }
   ],
   "source": [
    "# Check the top rows of the tsv file\n",
    "!head /var/lib/neo4j/import/clickstream-enwiki-2018-12_clean.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Connect this notebook to neo4j  \n",
    "\n",
    "We can use the `py2neo` library to connect from python to a neo4j instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py2neo import authenticate, Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid typing the neo4j password into the notebook each time, we can save it in a separate file and read it in with the helper function. There are many ways to do this.  \n",
    "For this project, we'll create a file called `.n4jpass` in the current folder, with the following contents:  \n",
    "```\n",
    "# comments here\n",
    "user=neo4j\n",
    "password=secretStuff123\n",
    "``` \n",
    "(Just replace the password in the above with your actual neo4j password.)  \n",
    "\n",
    "The helper function to read in the neo4j username and password is called `read_n4jpass`, and it's in the `custom_utils.py` module in the current folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the neo4j credentials from .n4jpass\n",
    "n4j_cred = read_n4jpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up authentication parameters\n",
    "authenticate(\"localhost:7474\", n4j_cred[\"user\"], n4j_cred[\"password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to authenticated graph database\n",
    "graph = Graph(\"http://localhost:7474/db/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Set indices  \n",
    "\n",
    "**Note:** do this step BEFORE importing the data!  \n",
    "\n",
    "Unlike relational databases, there are no built-in constraints or automatic indexing in neo4j. Since we expect Wikipedia article names to be unique, let's add a uniqueness constraint. This will make the import queries run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"CREATE CONSTRAINT ON (a:Article) ASSERT a.title IS UNIQUE;\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>failureMessage</th>\n",
       "      <th>id</th>\n",
       "      <th>indexName</th>\n",
       "      <th>progress</th>\n",
       "      <th>properties</th>\n",
       "      <th>provider</th>\n",
       "      <th>state</th>\n",
       "      <th>tokenNames</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INDEX ON :Article(title)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>index_1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>[title]</td>\n",
       "      <td>{'version': '1.0', 'key': 'native-btree'}</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>[Article]</td>\n",
       "      <td>node_unique_property</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                description failureMessage  id indexName  progress properties  \\\n",
       "0  INDEX ON :Article(title)                  1   index_1     100.0    [title]   \n",
       "\n",
       "                                    provider   state tokenNames  \\\n",
       "0  {'version': '1.0', 'key': 'native-btree'}  ONLINE  [Article]   \n",
       "\n",
       "                   type  \n",
       "0  node_unique_property  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all indices in the db\n",
    "r = graph.data('CALL db.indexes;')\n",
    "pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Load in the data  \n",
    "\n",
    "There are a few ways to load data into neo4j. Here, we'll use `LOAD CSV` with `PERIODIC COMMIT` to load the data.  \n",
    "\n",
    "##### Check data read-in  \n",
    "The `LOAD CSV` statement allows us to perform some data parsing at load time. We can check what the incoming data rows look like to the `LOAD CSV` statement and whether the data is being parsed correctly by using the `LOAD CSV` with `RETURN` and `LIMIT` statements, without writing anything to the database yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"clickstream-enwiki-2018-12_clean.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[other-empty, 2019_Horizon_League_Baseball_Tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[other-search, ForeverAtLast, external, 40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[other-empty, ForeverAtLast, external, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[First_Families_of_Pakistan, Jehangir_Wadia, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The_Lawrence_School,_Sanawar, Jehangir_Wadia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Wadia_family, Jehangir_Wadia, link, 715]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[other-search, Jehangir_Wadia, external, 967]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Ness_Wadia, Jehangir_Wadia, link, 494]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[other-empty, Jehangir_Wadia, external, 638]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[GoAir, Jehangir_Wadia, link, 1191]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 row\n",
       "0  [other-empty, 2019_Horizon_League_Baseball_Tou...\n",
       "1        [other-search, ForeverAtLast, external, 40]\n",
       "2         [other-empty, ForeverAtLast, external, 85]\n",
       "3  [First_Families_of_Pakistan, Jehangir_Wadia, l...\n",
       "4  [The_Lawrence_School,_Sanawar, Jehangir_Wadia,...\n",
       "5          [Wadia_family, Jehangir_Wadia, link, 715]\n",
       "6      [other-search, Jehangir_Wadia, external, 967]\n",
       "7            [Ness_Wadia, Jehangir_Wadia, link, 494]\n",
       "8       [other-empty, Jehangir_Wadia, external, 638]\n",
       "9                [GoAir, Jehangir_Wadia, link, 1191]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = 'file:///' + filename\n",
    "    \n",
    "query_test = \"\"\"\n",
    "    LOAD CSV FROM {myfilepath} AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    RETURN row\n",
    "    LIMIT 10\n",
    "    ;\n",
    "    \"\"\"\n",
    "\n",
    "test = graph.data(query_test, myfilepath=str(fp))\n",
    "pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row[0]</th>\n",
       "      <th>row[1]</th>\n",
       "      <th>split(row[0], '-')[1]</th>\n",
       "      <th>toInteger(row[3])</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>2019_Horizon_League_Baseball_Tournament</td>\n",
       "      <td>empty</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other-search</td>\n",
       "      <td>ForeverAtLast</td>\n",
       "      <td>search</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>ForeverAtLast</td>\n",
       "      <td>empty</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First_Families_of_Pakistan</td>\n",
       "      <td>Jehangir_Wadia</td>\n",
       "      <td>None</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The_Lawrence_School,_Sanawar</td>\n",
       "      <td>Jehangir_Wadia</td>\n",
       "      <td>None</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wadia_family</td>\n",
       "      <td>Jehangir_Wadia</td>\n",
       "      <td>None</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other-search</td>\n",
       "      <td>Jehangir_Wadia</td>\n",
       "      <td>search</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ness_Wadia</td>\n",
       "      <td>Jehangir_Wadia</td>\n",
       "      <td>None</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other-empty</td>\n",
       "      <td>Jehangir_Wadia</td>\n",
       "      <td>empty</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GoAir</td>\n",
       "      <td>Jehangir_Wadia</td>\n",
       "      <td>None</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         row[0]                                   row[1]  \\\n",
       "0                   other-empty  2019_Horizon_League_Baseball_Tournament   \n",
       "1                  other-search                            ForeverAtLast   \n",
       "2                   other-empty                            ForeverAtLast   \n",
       "3    First_Families_of_Pakistan                           Jehangir_Wadia   \n",
       "4  The_Lawrence_School,_Sanawar                           Jehangir_Wadia   \n",
       "5                  Wadia_family                           Jehangir_Wadia   \n",
       "6                  other-search                           Jehangir_Wadia   \n",
       "7                    Ness_Wadia                           Jehangir_Wadia   \n",
       "8                   other-empty                           Jehangir_Wadia   \n",
       "9                         GoAir                           Jehangir_Wadia   \n",
       "\n",
       "  split(row[0], '-')[1]  toInteger(row[3])  \n",
       "0                 empty                 16  \n",
       "1                search                 40  \n",
       "2                 empty                 85  \n",
       "3                  None                 19  \n",
       "4                  None                 36  \n",
       "5                  None                715  \n",
       "6                search                967  \n",
       "7                  None                494  \n",
       "8                 empty                638  \n",
       "9                  None               1191  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_test = \"\"\"\n",
    "    LOAD CSV FROM {myfilepath} AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    RETURN row[0], row[1], split(row[0], '-')[1], toInteger(row[3])\n",
    "    LIMIT 10\n",
    "    ;\n",
    "    \"\"\"\n",
    "\n",
    "test = graph.data(query_test, myfilepath=str(fp))\n",
    "pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load nodes first  \n",
    "\n",
    "When edges are being created in neo4j, they either connect existing nodes or create any non-existing nodes that are part of the edges, which can be very slow. Loading just the nodes first can significantly speed up the overall data import.  \n",
    "\n",
    "The English Wikipedia dataset is very large, and loading it into neo4j from this notebook using `py2neo` turned out to be too slow. Instead, we'll load the data using `cypher-shell` in terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### cypher-shell queries  \n",
    "\n",
    "These queries were run by running `cypher-shell` in terminal on AWS, logging in with neo4j credentials, and copying+pasting the scripts below, one at a time. A less manual way to run these queries is to save them to a text file and pipe them to `cypher-shell`.  \n",
    "\n",
    "Because the raw data file is very large, the nodes/edges read-ins are split up by reference types to create smaller file chunks, to make it easier to test the results.  \n",
    "\n",
    "**External reference type subsets, by referer category** \n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[0] = 'other-empty' \n",
    "    CREATE (n:Article { title: row[1], empty_referer_traffic:  toInteger(row[3]) })\n",
    "    ;\n",
    "```  \n",
    "_Added 5093433 nodes, Set 10186866 properties, Added 5093433 labels_  \n",
    "The above ran for about 5 min in terminal cypher-shell on AWS.  \n",
    "\n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[0] = 'other-external' \n",
    "    MERGE (n:Article { title: row[1] })\n",
    "    ON CREATE SET n.external_website_traffic = toInteger(row[3])\n",
    "    ON MATCH SET n.external_website_traffic = toInteger(row[3])\n",
    "    ;\n",
    "```  \n",
    "_Added 726 nodes, Set 788037 properties, Added 726 labels_  \n",
    "The above ran for about 2 min in terminal cypher-shell on AWS.  \n",
    "\n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[0] = 'other-other' \n",
    "    MERGE (n:Article { title: row[1] })\n",
    "    ON CREATE SET n.unknown_external_traffic = toInteger(row[3])\n",
    "    ON MATCH SET n.unknown_external_traffic = toInteger(row[3])\n",
    "    ;\n",
    "```  \n",
    "_Added 31 nodes, Set 374952 properties, Added 31 labels_  \n",
    "The above ran for about 2 min in terminal cypher-shell on AWS.  \n",
    "\n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[0] = 'other-internal' \n",
    "    MERGE (n:Article { title: row[1] })\n",
    "    ON CREATE SET n.other_wikimedia_traffic = toInteger(row[3])\n",
    "    ON MATCH SET n.other_wikimedia_traffic = toInteger(row[3])\n",
    "    ;\n",
    "```  \n",
    "_Added 3454 nodes, Set 1352346 properties, Added 3454 labels_  \n",
    "The above ran for about 2 min in terminal cypher-shell on AWS.  \n",
    "\n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[0] = 'other-search' \n",
    "    MERGE (n:Article { title: row[1] })\n",
    "    ON CREATE SET n.external_search_traffic = toInteger(row[3])\n",
    "    ON MATCH SET n.external_search_traffic = toInteger(row[3])\n",
    "    ;\n",
    "```  \n",
    "_Added 65465 nodes, Set 3451890 properties, Added 65465 labels_  \n",
    "The above ran for about 3.5 min in terminal cypher-shell on AWS.\n",
    "\n",
    "\n",
    "\n",
    "**Non-external reference type subset**\n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[2] <> 'external' \n",
    "    MERGE (n1:Article { title: row[0] })\n",
    "    MERGE (n2:Article { title: row[1] })\n",
    "    ;\n",
    "```  \n",
    "_Added 22590 nodes, Set 22590 properties, Added 22590 labels_  \n",
    "The above ran for about 13 min in terminal cypher-shell on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load edges/relationships  \n",
    "In graphs, edges=relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**LINK_TO relationships**  \n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[2] = 'link' \n",
    "    MATCH (n1:Article { title: row[0] })\n",
    "    MATCH (n2:Article { title: row[1] })\n",
    "    CREATE (n1)-[r:LINK_TO { traffic: toInteger(row[3]) }]->(n2)\n",
    "    ;\n",
    "```  \n",
    "_Created 17851574 relationships, Set 17851574 properties_  \n",
    "The above ran for about 26 min in terminal cypher-shell on AWS.\n",
    "\n",
    "\n",
    "**SEARCH_FOR relationships**  \n",
    "```\n",
    "USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM \"file:///clickstream-enwiki-2018-12_clean.tsv\" AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[2] = 'other' \n",
    "    MATCH (n1:Article { title: row[0] })\n",
    "    MATCH (n2:Article { title: row[1] })\n",
    "    CREATE (n1)-[r:SEARCH_FOR { traffic: toInteger(row[3]) }]->(n2)\n",
    "    ;\n",
    "```  \n",
    "_Created 1005180 relationships, Set 1005180 properties_  \n",
    "The above ran for about 14 min in terminal cypher-shell on AWS.  \n",
    "\n",
    "\n",
    "Total runtime was about 70 min on AWS cypher-shell. Every other method of import was much much slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Check the loaded database  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cypher queries for inspecting the loaded data in the neo4j browser. The visualizations are exported from neo4j browser.\n",
    "\n",
    "Visualize the database schema  \n",
    "```\n",
    "call db.schema.visualization()  \n",
    "```  \n",
    "Output:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/EN_wiki_1218_schema.png\" alt=\"neo4j db schema\" width=\"20%\" style=\"margin-left: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 5.6 Adress outliers  \n",
    "\n",
    "Fix any article linkings that likely do not represent the user behavior we're trying to model.  \n",
    "The queries below were run and visualized in neo4j browser.\n",
    "\n",
    "##### \"Main Page\" article out-links  \n",
    "```\n",
    "match p=(a:Article)-[r:LINK_TO]->()\n",
    "where a.title=\"Main_Page\"\n",
    "return p\n",
    "limit 200  \n",
    "```  \n",
    "Output:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/en_1218_Main_Page_outgoing_links_graph.png\" alt=\"Main Page out-links graph\" width=\"60%\" style=\"margin-left: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### \"Main Page\" in-links\n",
    "```\n",
    "match p=(a:Article)<-[r:LINK_TO]-()\n",
    "where a.title=\"Main_Page\"\n",
    "return p\n",
    "limit 200  \n",
    "```  \n",
    "Output:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/en_1218_Main_Page_incoming_links_graph.png\" alt=\"Main Page in-links graph\" width=\"60%\" style=\"margin-left: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### \"Main Page\" out-searches\n",
    "```\n",
    "match p=(a:Article)-[r:SEARCH_FOR]->()\n",
    "where a.title=\"Main_Page\"\n",
    "return p\n",
    "limit 200  \n",
    "```\n",
    "Output:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/en_1218_Main_Page_outgoing_search_graph_lim200.png\" alt=\"Main Page out-searches graph\" width=\"60%\" style=\"margin-left: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### \"Main Page\" in-searches  \n",
    "```\n",
    "match p=(a:Article)<-[r:SEARCH_FOR]-()\n",
    "where a.title=\"Main_Page\"\n",
    "return p\n",
    "limit 200  \n",
    "```\n",
    "Output:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/en_1218_Main_Page_incoming_search_graph_lim200.png\" alt=\"Main Page in-searches graph\" width=\"60%\" style=\"margin-left: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Conclusion:** Keep the link traffic to the \"Main Page\" article, because it seems to be focused on a specific topic and does not overwhelm the graph, but drop the link traffic from \"Main Page\" and the to/from search traffic, because those relationships are likely to overlink any topic-specific browsing behaviors. We'll save the dropped \"Main Page\" article traffic values on nodes.  \n",
    "\n",
    "\n",
    "cypher-shell query to remove Main_Page out-link-traffic:  \n",
    "```\n",
    "MATCH (mp:Article {title: \"Main_Page\"})-[r:LINK_TO]->(a:Article)\n",
    "SET a.link_traffic_from_main_page = r.traffic\n",
    "DELETE r;  \n",
    "```  \n",
    "_0 rows available after 141 ms, consumed after another 0 ms  \n",
    "Deleted 73 relationships, Set 73 properties_  \n",
    "\n",
    "\n",
    "cypher-shell query to remove Main_Page out-search-traffic:  \n",
    "```\n",
    "MATCH (mp:Article {title: \"Main_Page\"})-[r:SEARCH_FOR]->(a:Article)\n",
    "SET a.search_traffic_from_main_page = r.traffic\n",
    "DELETE r;  \n",
    "```   \n",
    "_0 rows available after 30816 ms, consumed after another 0 ms  \n",
    "Deleted 257794 relationships, Set 257794 properties_  \n",
    "\n",
    "\n",
    "cypher-shell query to remove Main_Page in-search-traffic:  \n",
    "```\n",
    "MATCH (mp:Article {title: \"Main_Page\"})<-[r:SEARCH_FOR]-(a:Article)\n",
    "SET a.search_traffic_to_main_page = r.traffic\n",
    "DELETE r;  \n",
    "```  \n",
    "_0 rows available after 1427 ms, consumed after another 0 ms  \n",
    "Deleted 110009 relationships, Set 110009 properties_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### \"Hyphen-minus\" article out-searches  \n",
    "```\n",
    "match p=(a:Article)-[r:SEARCH_FOR]->()\n",
    "where a.title=\"Hyphen-minus\"\n",
    "return p\n",
    "limit 200\n",
    "```\n",
    "The only out-search from \"Hyphen-minus\" was to the \"Main Page\" article, which was deleted in the queries above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### \"Hyphen-minus\" in-searches  \n",
    "```\n",
    "match p=(a:Article)<-[r:SEARCH_FOR]-()\n",
    "where a.title=\"Hyphen-minus\"\n",
    "return p\n",
    "limit 200\n",
    "```  \n",
    "\n",
    "Output:  \n",
    "\n",
    "<div>\n",
    "<img src=\"img/en_1218_Hyphen_minus_incoming_search_graph_lim200.png\" alt=\"Hyphen-minus in-searches graph\" width=\"60%\" style=\"margin-left: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Conclusion:** Drop the incoming search traffic to the \"Hyphen-minus\" article. We probably don't need to save the dropped \"Hyphen-minus\" traffic values on nodes, but doing it here just in case it looks interesting later.  \n",
    "\n",
    "cypher-shell query to remove \"Hyphen-minus\" in-search-traffic:  \n",
    "```\n",
    "MATCH (hm:Article {title: \"Hyphen-minus\"})<-[r:SEARCH_FOR]-(a:Article)\n",
    "SET a.search_traffic_to_hyphen_minus = r.traffic\n",
    "DELETE r;  \n",
    "```  \n",
    "_0 rows available after 1569 ms, consumed after another 0 ms  \n",
    "Deleted 127457 relationships, Set 127457 properties_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### \"Undefined\" article out-searches  \n",
    "```\n",
    "match p=(a:Article)-[r:SEARCH_FOR]->()\n",
    "where a.title=\"Undefined\"\n",
    "return p\n",
    "limit 200\n",
    "```  \n",
    "The only out-search from the \"Undefined\" article was to the \"Main Page\" article, which was deleted in the queries above.  \n",
    "\n",
    "##### \"Undefined\" article in-searches  \n",
    "```\n",
    "match p=(a:Article)<-[r:SEARCH_FOR]-()\n",
    "where a.title=\"Undefined\"\n",
    "return p\n",
    "limit 200\n",
    "```  \n",
    "Output:  \n",
    "\n",
    "<div>\n",
    "<img src=\"img/en_1218_Undefined_incoming_search_graph_lim200.png\" alt=\"Undefined in-searches graph\" width=\"60%\" style=\"margin-left: 0;\">\n",
    "</div>\n",
    "\n",
    "**Conclusion:** Drop the incoming search traffic to the \"Undefined\" article. We probably don't need to save the dropped \"Undefined\" traffic values on nodes, but doing it here just in case it looks interesting later.  \n",
    "\n",
    "cypher-shell query to remove \"Undefined\" in-search-traffic:  \n",
    "```\n",
    "MATCH (u:Article {title: \"Undefined\"})<-[r:SEARCH_FOR]-(a:Article)\n",
    "SET a.search_traffic_to_undefined = r.traffic\n",
    "DELETE r;  \n",
    "```  \n",
    "_0 rows available after 37 ms, consumed after another 0 ms\n",
    "Deleted 241 relationships, Set 241 properties_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's get a sample of the resulting graph paths in neo4j browser:  \n",
    "```\n",
    "MATCH p=()-[]->() RETURN p LIMIT 300\n",
    "```  \n",
    "Output:  \n",
    "\n",
    "<div>\n",
    "<img src=\"img/en_1218_final_graph_sample300.png\" alt=\"Final graph sample\" width=\"90%\" style=\"margin-left: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The graph sample above looks good, with nodes forming clusters, and no overlinking overwhelming the graph.  \n",
    "This Wikipedia clickstream graph can now be used to find influential articles and detect article communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Summary  \n",
    "\n",
    "In this notebook, we've defined the graph model for the English Wikipedia clickstream data, set up a neo4j graph database on AWS EC2, and imported the data according to the graph model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. Next steps   \n",
    "\n",
    "Now that we have the clickstream data stored as a graph, in the [next notebook](English_Wikipedia_network_analysis_AWS.ipynb) we'll use network analysis tenchniques to describe the relationship structures within the data, identify infuential nodes and detect article communities via user browsing behavior. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
