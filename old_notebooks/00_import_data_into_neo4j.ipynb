{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Wikipedia clickstream data into neo4j graph database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In 2015, Wikipedia started releasing datasets of clickstream counts to Wikipedia articles for research purposes. The project has evolved over time, and as of April 2018 it takes the form of monthly automatic releases in 11 Wikipedia language domains: English, German, French, Spanish, Russian, Italian, Japanese, Portuguese, Polish, Chinese and Persian.\n",
    "\n",
    "The clickstream datasets contain counts of times someone went to a Wikipedia article from either another Wikipedia article or from some other webpage. The destination Wikipedia article here is called a resource, and the webpage from which the user went to the resource is called a referer. Referers can be another Wikipedia article from the same language domain, a Wikimedia page that is not part of that Wikipedia language domain, a search engine, or some other webpage. If the referer is a Wikipedia article from the same language domain, then the article name is given. If not, then a general referer category is given.\n",
    "\n",
    "**Some examples of the data records:**\n",
    "1. _Article-to-article data record_  \n",
    "In the English Wikipedia, the article 'Suki Waterhouse' links to the article 'List of Divergent characters'. In March 2018, users went from the 'Suki Waterhouse' article to the 'List of Divergent characters' article 86 times. This clickstream activity between the two articles is recorded in the 2018-03 English Wikipedia clickstream data release as:  \n",
    "\n",
    "referer | resource | reference type | count\n",
    "--- | --- | --- | ---  \n",
    "Suki_Waterhouse | List_of_Divergent_characters | link | 86\n",
    "\n",
    "2. _External-source-to-article data record_  \n",
    "In the English Wikipedia, the article 'Bureau of Investigative Journalism' was visited 18 times during March 2018 from some other Wikimedia page that is not part of the English Wikipedia. This clickstream activity is recorded in the 2018-03 English Wikipedia clickstream data release as:  \n",
    "\n",
    "referer | resource | reference type | count\n",
    "--- | --- | --- | ---  \n",
    "other-internal | Bureau_of_Investigative_Journalism | external | 18\n",
    "\n",
    "---\n",
    "\n",
    "The Wikipedia clickstream data shows how users get to Wikipedia articles per language domain. It is a weighted network of articles and external sources, weighted by the number of times users went to a given Wikipedia article from either another Wikipedia article or some other webpage.\n",
    "\n",
    "\n",
    "**Reference for the raw data:**\n",
    "- Data source/download: [Wikipeadia clickstream data dumps](https://dumps.wikimedia.org/other/clickstream/)\n",
    "- Data description: [Research:Wikipedia clickstream](https://meta.wikimedia.org/wiki/Research:Wikipedia_clickstream) \n",
    "- The data is released and maintained by the Wikimedia Foundation's [Analytics Engineering team](https://wikitech.wikimedia.org/wiki/Analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data subset for this project\n",
    "\n",
    "For this project I'm using the March 2018 Wikipedia clickstream data release, and all of the 11 language domains provided. I'm restricting the data subset to March 2018 for two reasons. First, the way the Wikipedia clickstream data is processed has evolved significantly over time, and the number of language domains included in the releases has changed as well. The changes in data processing are likely to affect the data analysis results, so I am using the March 2018 subset in order to avoid that. Second, the Wikipedia clickstream data is quite large, and the March 2018 subset gives me enough data for an initial exploration. \n",
    "\n",
    "**Note on data size**  \n",
    "The English Wikipedia clickstream data file for March 2018 unzips to 1.3G. The second largest March 2018 data file is for the German Wikipedia clickstream, and it unzips to 219M. The data files for the other language domains are much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Data import\n",
    "\n",
    "Since the Wikipedia clickstream data is a weighted network of articles and other webpages, we can use a graph database to store this data as connections between webpages. Once we have this data in a graph database, we can leverage its structure to investigate the data with network analysis techniques.\n",
    "\n",
    "For this project, I am using the [neo4j](https://neo4j.com/) graph database, which I've set up on [AWS](https://aws.amazon.com/) cloud server. In the cells below, I'm connecting to my neo4j database instance from a jupyter notebook that I am running on the same AWS server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the data\n",
    "from the [2018-03 Wikipedia clickstream release](https://dumps.wikimedia.org/other/clickstream/2018-03/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data for import\n",
    "Neo4j interprets quotes as field terminators, even if another field terminator is explicitly specified (and backslashes in the data are also problematic). Checking the raw data for special characters and escaping them with a backslash solves this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on a data model\n",
    "\n",
    "There are many ways to model connected data as a graph. The goal is to create a graph database model that is easy to understand, and that provides fast traversal of relevant data. These goals are highly dependent on the business use case for the data.  \n",
    "\n",
    "For this project, I am interested in connections between Wikipedia articles, the role of external sources as referers, and differences and similarities between the Wikipedia articles networks across different language domains. So I am going to model the Wikipedia clickstream data in the following way:\n",
    "1. nodes\n",
    "   - Wikipedia article\n",
    "   - external reference source (i.e. not a Wikipedia article in the same language)\n",
    "   - both of the above node types will have language properties\n",
    "2. relationship\n",
    "   - 'REFERRED_TO'\n",
    "   - articles can refer to articles\n",
    "   - external sources can refer to articles\n",
    "   - this relationship will have 2 properties:\n",
    "     - count of the number of times users went from the referring webpage to the referred article in the data subset\n",
    "     - reference type: link (Wikipedia article links to article), external (non-Wikipedia referer), other (referer is a Wikipedia article, but the user did not click on a link to get to the referred article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py2neo import authenticate, Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up authentication parameters\n",
    "authenticate(\"localhost:7474\", \"my_neo4j_user\", \"my_neo4j_password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to authenticated graph database\n",
    "graph = Graph(\"http://localhost:7474/db/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set indices\n",
    "Unlike in relational databases, there are no built-in constraints or automatic indexing in neo4j. Let's add some indexes for our data, so that the queries below run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py2neo.database.Cursor at 0x7efbf49ed710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run('CREATE INDEX ON :Article(title);')\n",
    "graph.run('CREATE INDEX ON :Article(title, language_code);')\n",
    "graph.run('CREATE INDEX ON :ExternalSource(source_type);')\n",
    "graph.run('CREATE INDEX ON :ExternalSource(source_type, language_code);')\n",
    "graph.run('CREATE INDEX ON :Article(language_code);')\n",
    "graph.run('CREATE INDEX ON :ExternalSource(language_code);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list all indices in the db\n",
    "r = graph.data('CALL db.indexes;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "      <th>properties</th>\n",
       "      <th>provider</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INDEX ON :Article(language_code)</td>\n",
       "      <td>Article</td>\n",
       "      <td>[language_code]</td>\n",
       "      <td>{'version': '1.0', 'key': 'lucene+native'}</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>node_label_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INDEX ON :Article(title)</td>\n",
       "      <td>Article</td>\n",
       "      <td>[title]</td>\n",
       "      <td>{'version': '1.0', 'key': 'lucene+native'}</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>node_label_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDEX ON :Article(title, language_code)</td>\n",
       "      <td>Article</td>\n",
       "      <td>[title, language_code]</td>\n",
       "      <td>{'version': '1.0', 'key': 'lucene+native'}</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>node_label_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INDEX ON :ExternalSource(language_code)</td>\n",
       "      <td>ExternalSource</td>\n",
       "      <td>[language_code]</td>\n",
       "      <td>{'version': '1.0', 'key': 'lucene+native'}</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>node_label_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INDEX ON :ExternalSource(source_type)</td>\n",
       "      <td>ExternalSource</td>\n",
       "      <td>[source_type]</td>\n",
       "      <td>{'version': '1.0', 'key': 'lucene+native'}</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>node_label_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INDEX ON :ExternalSource(source_type, language...</td>\n",
       "      <td>ExternalSource</td>\n",
       "      <td>[source_type, language_code]</td>\n",
       "      <td>{'version': '1.0', 'key': 'lucene+native'}</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>node_label_property</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description           label  \\\n",
       "0                   INDEX ON :Article(language_code)         Article   \n",
       "1                           INDEX ON :Article(title)         Article   \n",
       "2            INDEX ON :Article(title, language_code)         Article   \n",
       "3            INDEX ON :ExternalSource(language_code)  ExternalSource   \n",
       "4              INDEX ON :ExternalSource(source_type)  ExternalSource   \n",
       "5  INDEX ON :ExternalSource(source_type, language...  ExternalSource   \n",
       "\n",
       "                     properties                                    provider  \\\n",
       "0               [language_code]  {'version': '1.0', 'key': 'lucene+native'}   \n",
       "1                       [title]  {'version': '1.0', 'key': 'lucene+native'}   \n",
       "2        [title, language_code]  {'version': '1.0', 'key': 'lucene+native'}   \n",
       "3               [language_code]  {'version': '1.0', 'key': 'lucene+native'}   \n",
       "4                 [source_type]  {'version': '1.0', 'key': 'lucene+native'}   \n",
       "5  [source_type, language_code]  {'version': '1.0', 'key': 'lucene+native'}   \n",
       "\n",
       "    state                 type  \n",
       "0  ONLINE  node_label_property  \n",
       "1  ONLINE  node_label_property  \n",
       "2  ONLINE  node_label_property  \n",
       "3  ONLINE  node_label_property  \n",
       "4  ONLINE  node_label_property  \n",
       "5  ONLINE  node_label_property  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpd = pd.DataFrame(r)\n",
    "rpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the import queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_wiki_to_neo(language, language_code):\n",
    "    # file path\n",
    "    language_code = language_code.upper()\n",
    "    language_code_fp = language_code.lower()\n",
    "    # note: by default, neo4j only imports data from its own import subfolder or from a url,\n",
    "    # so make sure to place the cleaned data into the neo4j import folder.\n",
    "    fp = 'file:///wikipedia_clickstream/clickstream-' + language_code_fp + 'wiki-2018-03_m.tsv'\n",
    "    \n",
    "    # Let's load article-to-article links and 'other' referrer types (all referrers here are Wiki articles)\n",
    "    # (it's faster to load nodes and relationships separately for large data files)\n",
    "    # nodes - internal\n",
    "    query_nodes_internal = \"\"\"\n",
    "    USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM {myfilepath} AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[2] <> 'external'\n",
    "    MERGE (n1:Article {title: row[0], language_code: {lang_code} })\n",
    "        ON CREATE SET\n",
    "            n1.language = {lang} \n",
    "    MERGE (n2:Article {title: row[1], language_code: {lang_code} })\n",
    "        ON CREATE SET\n",
    "            n2.language = {lang}\n",
    "    ;\n",
    "    \"\"\"\n",
    "    graph.run(query_nodes_internal, myfilepath=str(fp), lang=str(language), lang_code=str(language_code))\n",
    "\n",
    "    # relationships - internal\n",
    "    query_relationships_internal = \"\"\"\n",
    "    USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM {myfilepath} AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[2] <> 'external'\n",
    "    MATCH (n1:Article {title: row[0], language_code: {lang_code} })\n",
    "    MATCH (n2:Article {title: row[1], language_code: {lang_code} })\n",
    "    MERGE (n1)-[r:REFERRED_TO]->(n2)\n",
    "        ON CREATE SET\n",
    "            r.type = row[2],\n",
    "            r.count = row[3]\n",
    "    ;\n",
    "    \"\"\"\n",
    "    graph.run(query_relationships_internal, myfilepath=str(fp), lang=str(language), lang_code=str(language_code))\n",
    "\n",
    "    # nodes - external\n",
    "    query_nodes_external = \"\"\"\n",
    "    USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM {myfilepath} AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[2] = 'external'\n",
    "    MERGE (n1:ExternalSource {source_type: row[0], language_code: {lang_code} })\n",
    "        ON CREATE SET\n",
    "            n1.language = {lang},\n",
    "            n1.description = CASE \n",
    "                WHEN row[0] = 'other-internal' THEN 'a page from any other Wikimedia project (not Wikipedia)'\n",
    "                WHEN row[0] = 'other-search' THEN 'an external search engine'\n",
    "                WHEN row[0] = 'other-external' THEN 'any other external site (not search engine)'\n",
    "                WHEN row[0] = 'other-empty' THEN 'an empty referer'\n",
    "                WHEN row[0] = 'other-other' THEN 'anything else (catch-all)'\n",
    "                ELSE '' END\n",
    "\n",
    "    MERGE (n2:Article {title: row[1], language_code: {lang_code} })\n",
    "        ON CREATE SET\n",
    "            n2.language = {lang}\n",
    "    ;\n",
    "    \"\"\"\n",
    "    graph.run(query_nodes_external, myfilepath=str(fp), lang=str(language), lang_code=str(language_code))\n",
    "\n",
    "    # relationships - external\n",
    "    query_relationships_external = \"\"\"\n",
    "    USING PERIODIC COMMIT\n",
    "    LOAD CSV FROM {myfilepath} AS row\n",
    "    FIELDTERMINATOR '\\t'\n",
    "    WITH row\n",
    "    WHERE row[2] = 'external'\n",
    "    MATCH (n1:ExternalSource {source_type: row[0], language_code: {lang_code} })\n",
    "    MATCH (n2:Article {title: row[1], language_code: {lang_code} })\n",
    "    MERGE (n1)-[r:REFERRED_TO]->(n2)\n",
    "        ON CREATE SET\n",
    "            r.type = row[2],\n",
    "            r.count = row[3]\n",
    "    ;\n",
    "    \"\"\"\n",
    "    graph.run(query_relationships_external, myfilepath=str(fp), lang=str(language), lang_code=str(language_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These larger imports were faster to run in cypher-shell as cypher queries\n",
    "# import_wiki_to_neo('German', 'DE')\n",
    "# import_wiki_to_neo('English', 'EN')\n",
    "# import_wiki_to_neo('Spanish', 'ES')\n",
    "# import_wiki_to_neo('Persian', 'FA')\n",
    "# import_wiki_to_neo('Russian', 'RU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import_wiki_to_neo('French', 'FR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_wiki_to_neo('Italian', 'IT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import_wiki_to_neo('Japanese', 'JA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import_wiki_to_neo('Polish', 'PL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import_wiki_to_neo('Portuguese', 'PT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import_wiki_to_neo('Chinese', 'ZH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Viewing the graph database schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the database schema, run `CALL db.schema()` in the neo4j browser.  \n",
    "Here's the Wikipedia clickstream db schema:\n",
    "\n",
    "![Wikipedia clickstream database schema diagram](graph_db_schema.png \"Wikipedia clickstream database schema\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema diagram shows the types of nodes (things or entities) and edges (relationships) in the database.  \n",
    "  \n",
    "This schema diagram shows that we have two kinds of nodes: articles and external sources. Both of those nodes can have a directional relationship to article nodes, and that relationship is a reference from one node to the other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have our Wikipedia clickstream data loaded in a graph database, we can explore the data in the neo4j browser.\n",
    "Here is the data hairball I get when I run the following cypher query in the neo4j browser  \n",
    "`MATCH p=()-[r:REFERRED_TO]->(n:Article {language_code:'EN'}) RETURN p LIMIT 25`.  \n",
    "\n",
    "This query returns a random selection of 25 references to English Wikipedia articles. We see more than 25 relationships here, because in addition to the 25 references queried, neo4j returns all of the other relationships among the nodes involved in the 25 references. The numbers on the links are the reference counts. For example, users went to the 'Cara Delevingne' Wikipedia article from a search engine 186,252 times in March 2018 (bottom right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wikipedia clickstream subgraph](english_wiki_25_rels_graph.png \"English Wikipedia clickstream subgraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this subgraph, we can see that the clickstream data is highly interconnected. We can inspect closer a handful of nodes in the neo4j browser on a case by case basis, but to get a sense of what's going on in this graph as a whole, we need to use network analysis techniques.  \n",
    "We've loaded disjoint networks of Wikipedia articles for 11 language domains. Each one is a highly interconnected data hairball, like the example above. How can we compare them and quantify their differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
